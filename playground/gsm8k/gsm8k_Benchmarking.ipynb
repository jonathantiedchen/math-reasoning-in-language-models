{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba3f700",
   "metadata": {},
   "source": [
    "## Bechmarking using DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49570e",
   "metadata": {},
   "source": [
    "### Step 1: Initialize Model Class (required to conduct bechmarking with DeepEval )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd4b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/konstantinreicheneder/Desktop/Master_Thesis/math-reasoning-in-language-models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/konstantinreicheneder/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkreicheneder\u001b[0m (\u001b[33mkreicheneder-copenhagen-business-school\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact gpt2-math-model:v0, 479.31MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from Weights & Biases!\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login()\n",
    "\n",
    "# Load the latest model artifact from wandb\n",
    "run = wandb.init(project=\"master_thesis_math_lm\")\n",
    "artifact = run.use_artifact(\"master_thesis_math_lm/gpt2-math/gpt2-math-model:v0\", type=\"model\")\n",
    "artifact_dir = artifact.download()  # This will download the model to a local path\n",
    "\n",
    "# Load the model from the downloaded path\n",
    "model = AutoModelForCausalLM.from_pretrained(artifact_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "\n",
    "print(\"Model loaded successfully from Weights & Biases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfebe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a math joke or problem with physics? Let's think about it!\n",
      "\n",
      "The math joke.\n",
      "\n",
      "I don't know how they get on.\n",
      "\n",
      "What do your friends say about it?\n",
      "\n",
      "# 1.\n",
      "\n",
      "### Eulal and he are in love at the moment, the latter loves to watch. He goes to the cinema to buy popcorn and he notices that he was asked if there is any movie with the same title or the same director.\n",
      "\n",
      "#### Explanation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class FineTunedGPT2:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        \"\"\"Initialize with preloaded model and tokenizer.\"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Move model to the appropriate device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def generate(self, prompt: str, max_length=100) -> str:\n",
    "        \"\"\"Generates text from the fine-tuned GPT-2 model.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=max_length, do_sample=True)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Now, instead of reloading from wandb, pass your already loaded model & tokenizer\n",
    "gpt2_model = FineTunedGPT2(model, tokenizer)\n",
    "\n",
    "# Test text generation\n",
    "#print(gpt2_model.generate(\"Tell me a math joke\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2784e97",
   "metadata": {},
   "source": [
    "### Benchmarking on GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca91204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 175430.73 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 478447.37 examples/s]\n",
      "Processing 10 problems:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  10%|█         | 1/10 [00:02<00:22,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  20%|██        | 2/10 [00:04<00:19,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  30%|███       | 3/10 [00:07<00:17,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  40%|████      | 4/10 [00:09<00:14,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  50%|█████     | 5/10 [00:12<00:12,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  60%|██████    | 6/10 [00:14<00:10,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  70%|███████   | 7/10 [00:17<00:07,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  80%|████████  | 8/10 [00:19<00:04,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems:  90%|█████████ | 9/10 [00:22<00:02,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Processing 10 problems: 100%|██████████| 10/10 [00:25<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall GSM8K Accuracy: 0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from deepeval.benchmarks import GSM8K\n",
    "\n",
    "# Define benchmark with n_problems and shots\n",
    "benchmark = GSM8K(\n",
    "    n_problems=10,\n",
    "    n_shots=3,\n",
    "    enable_cot=True\n",
    ")\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "benchmark.evaluate(model=gpt2_model)\n",
    "print(benchmark.overall_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
