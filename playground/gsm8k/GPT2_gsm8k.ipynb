{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d1d7ec-aa7f-4086-a0ae-8b3af7261794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\n",
      "Reference answer: The discount price of one glass is 60/100 * 5 = $<<60/100*5=3>>3.\n",
      "If every second glass is cheaper, that means Kylar is going to buy 16 / 2 = <<16/2=8>>8 cheaper glasses.\n",
      "So for the cheaper glasses, Kylar is going to pay 8 * 3 = $<<8*3=24>>24.\n",
      "And for the regular-priced glasses, Kylar will pay 8 * 5 = $<<8*5=40>>40.\n",
      "So in total Kylar needs to pay 24 + 40 = $<<24+40=64>>64 for the glasses he wants to buy.\n",
      "#### 64\n",
      "Question: Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      " Let's think step by step. At the end, you MUST write the answer as an integer after '####'. Ensure your answer is fully written before stopping.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Master Thesis/math-reasoning-in-language-models/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Master Thesis/math-reasoning-in-language-models/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 Raw Solution:\n",
      "# 1 2 ############################ ####### ## ### .################################.. | \\| [^]$\\[A_][B_]'s' & \"`\"& \"\\{}\\\\\\\"\"\"@''\"'.''\"', `-',':''::\"'; \"'['']'/\\'; '[a:]')?'='(')'\"; \"[b]\"'-(?:)!'=\"''; \"\"@@@#%%%~%\" ; %%%%%!#$%. %%::%-<>%; ^^^//~~==.; !::::;;::::::::++.: : ::--.-.*-.*:., ,,,,.:,...-,....,-.....:-.(.)......```. ..,,,,................. ............. ........ .... .......... ........................................................ ... ................................................................ ?............. ||||||||&&.? \\\\ &&??!!?!!? !!!!!??? ??!!!!!!!!! ???!!!!!!!!?????????????????……….. ~---~~~~-----|~~~~~~~~===--------> ---- ------ --------- ------- -------- --- ----- ------------------------------- --====----- ===================------=============/ > <><></</span>\" >> </div>.\n",
      "\n",
      "GPT-2 Processed Solution:\n",
      "# 1 2 ####\n",
      "\n",
      "Extracted answer: 2.0\n",
      "Reference numeric answer: 64.0\n",
      "✗ Incorrect\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from utils.helper import get_device, extract_answer_gsm8k, create_cot_prompt\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also try \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "# set device based on machine used to run the code can be cuda, mps or cpu\n",
    "device = get_device()\n",
    "\n",
    "# Make sure padding token is set\n",
    "# GPT-2 doesn't have a pad token by default, so we use the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "test_set = gsm8k[\"test\"]\n",
    "train_set = gsm8k[\"train\"]\n",
    "\n",
    "# For demonstration, let's use a sample from GSM8K\n",
    "sample_idx = 5  # You can change this to try different examples\n",
    "sample = test_set[sample_idx]\n",
    "question = sample[\"question\"]\n",
    "reference_answer = sample[\"answer\"]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Reference answer: {reference_answer}\")\n",
    "\n",
    "# Create 5-shot prompt\n",
    "cot_prompt = create_cot_prompt(list(train_set), n_shot=5)\n",
    "prompt = f\"Question: {cot_prompt} Let's think step by step. At the end, you MUST write the answer as an integer after '####'. Ensure your answer is fully written before stopping.\\n\"\n",
    "print(prompt)\n",
    "\n",
    "# Generate answer with GPT-2\n",
    "# GPT-2 may need more specific generation parameters\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,  # Higher temperature for GPT-2 often helps with math\n",
    "    do_sample=False,\n",
    "    top_p=0.9,\n",
    "    top_k=50,  # Adding top-k filtering for better control\n",
    "    repetition_penalty=1.2,  # Help reduce repetitions\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(generated_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Post-process the response to handle repetition or early stopping\n",
    "def post_process_response(response):\n",
    "    # If response contains a #### followed by new content, trim it\n",
    "    if \"####\" in response:\n",
    "        parts = response.split(\"####\")\n",
    "        # Keep the content before #### and the first part after ####\n",
    "        main_answer = parts[0] + \"####\" + parts[1].split(\"\\n\\n\")[0]\n",
    "        return main_answer\n",
    "    return response\n",
    "\n",
    "# Apply post-processing\n",
    "processed_response = post_process_response(response)\n",
    "\n",
    "print(\"\\nGPT-2 Raw Solution:\")\n",
    "print(response)\n",
    "\n",
    "print(\"\\nGPT-2 Processed Solution:\")\n",
    "print(processed_response)\n",
    "\n",
    "# Extract the numerical answer for evaluation\n",
    "predicted_answer = extract_answer_gsm8k(processed_response)\n",
    "reference_numeric = extract_answer_gsm8k(reference_answer)\n",
    "\n",
    "print(f\"\\nExtracted answer: {predicted_answer}\")\n",
    "print(f\"Reference numeric answer: {reference_numeric}\")\n",
    "\n",
    "# Check if the answer is correct\n",
    "if predicted_answer is not None and reference_numeric is not None:\n",
    "    # Allow for small floating point differences\n",
    "    if abs(predicted_answer - reference_numeric) < 1e-6:\n",
    "        print(\"✓ Correct!\")\n",
    "    else:\n",
    "        print(\"✗ Incorrect\")\n",
    "else:\n",
    "    print(\"Could not extract a numeric answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb8ffa7-0970-4ef0-a8f6-a3e04f37edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8K dataset downloaded successfully.\n",
      "Loaded 7473 training examples and 1319 test examples\n",
      "Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\n",
      "Reference answer: The discount price of one glass is 60/100 * 5 = $<<60/100*5=3>>3.\n",
      "If every second glass is cheaper, that means Kylar is going to buy 16 / 2 = <<16/2=8>>8 cheaper glasses.\n",
      "So for the cheaper glasses, Kylar is going to pay 8 * 3 = $<<8*3=24>>24.\n",
      "And for the regular-priced glasses, Kylar will pay 8 * 5 = $<<8*5=40>>40.\n",
      "So in total Kylar needs to pay 24 + 40 = $<<24+40=64>>64 for the glasses he wants to buy.\n",
      "#### 64\n",
      "\n",
      "5-Shot Prompt (excerpt):\n",
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<1...\n",
      "...asses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\n",
      "Answer: Let's solve this step by step. I'll write the final answer after ####.\n",
      "\n",
      "\n",
      "GPT-2 Raw Solution:\n",
      "###Step 1: Create an account or create another page using any template we've created (see next steps). You will then be asked what type(s) should work best during Step 2; if so, choose \"Design\". To avoid confusion between design templates here, it makes sense not just use CSS classes like HTML class names—you also want something nice where there aren't too big blocks —but don`T do anything weird when defining styles suchas \"#draw_circle\" instead!\n",
      "\n",
      "GPT-2 Processed Solution:\n",
      "###Step 1: Create an account or create another page using any template we've created (see next steps). You will then be asked what type(s) should work best during Step 2; if so, choose \"Design\". To avoid confusion between design templates here, it makes sense not just use CSS classes like HTML class names—you also want something nice where there aren't too big blocks —but don`T do anything weird when defining styles suchas \"#draw_circle\" instead!\n",
      "\n",
      "Extracted answer: 2.0\n",
      "Reference numeric answer: 64.0\n",
      "✗ Incorrect\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also try \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "# Make sure padding token is set\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Function to download GSM8K dataset from GitHub\n",
    "def download_gsm8k():\n",
    "    import requests\n",
    "    import os\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(\"gsm8k_data\", exist_ok=True)\n",
    "    \n",
    "    # URLs for train and test data\n",
    "    train_url = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/train.jsonl\"\n",
    "    test_url = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl\"\n",
    "    \n",
    "    # Download train set\n",
    "    train_response = requests.get(train_url)\n",
    "    with open(\"gsm8k_data/train.jsonl\", \"wb\") as f:\n",
    "        f.write(train_response.content)\n",
    "    \n",
    "    # Download test set\n",
    "    test_response = requests.get(test_url)\n",
    "    with open(\"gsm8k_data/test.jsonl\", \"wb\") as f:\n",
    "        f.write(test_response.content)\n",
    "    \n",
    "    print(\"GSM8K dataset downloaded successfully.\")\n",
    "\n",
    "# Function to load the GSM8K dataset from files\n",
    "def load_gsm8k_from_file():\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    # Check if files exist, download if not\n",
    "    if not (os.path.exists(\"gsm8k_data/train.jsonl\") and os.path.exists(\"gsm8k_data/test.jsonl\")):\n",
    "        download_gsm8k()\n",
    "    \n",
    "    # Load train data\n",
    "    with open(\"gsm8k_data/train.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            train_data.append(json.loads(line))\n",
    "    \n",
    "    # Load test data\n",
    "    with open(\"gsm8k_data/test.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            test_data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Loaded {len(train_data)} training examples and {len(test_data)} test examples\")\n",
    "    return train_data, test_data\n",
    "\n",
    "# Load our GSM8K dataset\n",
    "train_set, test_set = load_gsm8k_from_file()\n",
    "\n",
    "# Function to extract the numerical answer\n",
    "def extract_answer(text):\n",
    "    if \"####\" in text:\n",
    "        answer_part = text.split(\"####\")[-1].strip()\n",
    "        numbers = re.findall(r'[-+]?\\d*\\.?\\d+', answer_part)\n",
    "        if numbers:\n",
    "            return float(numbers[0])\n",
    "    \n",
    "    # Fallback: extract the last number in the full text\n",
    "    numbers = re.findall(r'[-+]?\\d*\\.?\\d+', text)\n",
    "    if numbers:\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to create 5-shot prompt\n",
    "def create_five_shot_prompt(train_examples: List[Dict[str, Any]], question: str):\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Sample 5 examples from the training set\n",
    "    shot_examples = random.sample(train_examples, 5)\n",
    "    \n",
    "    # Build the few-shot prompt\n",
    "    prompt = \"\"\n",
    "    for ex in shot_examples:\n",
    "        prompt += f\"Question: {ex['question']}\\n\"\n",
    "        prompt += f\"Answer: {ex['answer']}\\n\\n\"\n",
    "    \n",
    "    # Add the current question with clear instructions for GPT-2\n",
    "    prompt += f\"Question: {question}\\n\"\n",
    "    prompt += f\"Answer: Let's solve this step by step. I'll write the final answer after ####.\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# For demonstration, let's use a sample from GSM8K\n",
    "sample_idx = 5  # You can change this to try different examples\n",
    "sample = test_set[sample_idx]\n",
    "question = sample[\"question\"]\n",
    "reference_answer = sample[\"answer\"]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Reference answer: {reference_answer}\")\n",
    "\n",
    "# Create 5-shot prompt\n",
    "five_shot_prompt = create_five_shot_prompt(train_set, question)\n",
    "\n",
    "print(\"\\n5-Shot Prompt (excerpt):\")\n",
    "# Print the beginning and end of the prompt if it's long\n",
    "if len(five_shot_prompt) > 500:\n",
    "    print(five_shot_prompt[:250] + \"...\\n...\" + five_shot_prompt[-250:])\n",
    "else:\n",
    "    print(five_shot_prompt)\n",
    "\n",
    "# Generate answer with GPT-2\n",
    "inputs = tokenizer(five_shot_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(generated_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Post-process the response to handle repetition or early stopping\n",
    "def post_process_response(response):\n",
    "    # If response contains a #### followed by new content, trim it\n",
    "    if \"####\" in response:\n",
    "        parts = response.split(\"####\")\n",
    "        # Keep the content before #### and the first part after ####\n",
    "        main_answer = parts[0] + \"####\" + parts[1].split(\"\\n\\n\")[0]\n",
    "        return main_answer\n",
    "    return response\n",
    "\n",
    "# Apply post-processing\n",
    "processed_response = post_process_response(response)\n",
    "\n",
    "print(\"\\nGPT-2 Raw Solution:\")\n",
    "print(response)\n",
    "\n",
    "print(\"\\nGPT-2 Processed Solution:\")\n",
    "print(processed_response)\n",
    "\n",
    "# Extract the numerical answer for evaluation\n",
    "predicted_answer = extract_answer(processed_response)\n",
    "reference_numeric = extract_answer(reference_answer)\n",
    "\n",
    "print(f\"\\nExtracted answer: {predicted_answer}\")\n",
    "print(f\"Reference numeric answer: {reference_numeric}\")\n",
    "\n",
    "# Check if the answer is correct\n",
    "if predicted_answer is not None and reference_numeric is not None:\n",
    "    # Allow for small floating point differences\n",
    "    if abs(predicted_answer - reference_numeric) < 1e-6:\n",
    "        print(\"✓ Correct!\")\n",
    "    else:\n",
    "        print(\"✗ Incorrect\")\n",
    "else:\n",
    "    print(\"Could not extract a numeric answer\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
