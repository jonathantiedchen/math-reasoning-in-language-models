{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to download the OpenWebMath dataset and save it locally\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def ensure_dir_exists(directory):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "def save_dataset_to_disk(dataset, output_dir, format=\"parquet\"):\n",
    "    \"\"\"\n",
    "    Save the dataset to disk in the specified format\n",
    "    Supported formats: parquet, json, csv\n",
    "    \"\"\"\n",
    "    ensure_dir_exists(output_dir)\n",
    "    \n",
    "    # Iterate through each split in the dataset\n",
    "    for split_name, split_data in dataset.items():\n",
    "        split_dir = os.path.join(output_dir, split_name)\n",
    "        ensure_dir_exists(split_dir)\n",
    "        \n",
    "        # Get the file path for the output\n",
    "        if format == \"parquet\":\n",
    "            file_path = os.path.join(split_dir, f\"{split_name}.parquet\")\n",
    "            # Save as parquet file (efficient for large datasets)\n",
    "            split_data.to_parquet(file_path)\n",
    "        elif format == \"json\":\n",
    "            file_path = os.path.join(split_dir, f\"{split_name}.json\")\n",
    "            # Convert to pandas and save as json\n",
    "            df = split_data.to_pandas()\n",
    "            df.to_json(file_path, orient=\"records\", lines=True)\n",
    "        elif format == \"csv\":\n",
    "            file_path = os.path.join(split_dir, f\"{split_name}.csv\")\n",
    "            # Convert to pandas and save as csv\n",
    "            df = split_data.to_pandas()\n",
    "            df.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "        \n",
    "        print(f\"Saved {split_name} split to {file_path}\")\n",
    "        \n",
    "        # Save a sample file with just a few examples for quick inspection\n",
    "        sample_file_path = os.path.join(split_dir, f\"{split_name}_sample.json\")\n",
    "        with open(sample_file_path, 'w') as f:\n",
    "            json.dump(split_data[:5], f, indent=2)\n",
    "        print(f\"Saved sample of {split_name} split to {sample_file_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_file = os.path.join(split_dir, \"metadata.json\")\n",
    "        metadata = {\n",
    "            \"num_examples\": len(split_data),\n",
    "            \"column_names\": split_data.column_names,\n",
    "            \"features\": str(split_data.features),\n",
    "        }\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"Saved metadata to {metadata_file}\")\n",
    "\n",
    "def main():\n",
    "    # Define the output directory\n",
    "    output_dir = \"data\"\n",
    "    \n",
    "    print(\"Downloading OpenWebMath dataset...\")\n",
    "    # Load the dataset\n",
    "    try:\n",
    "        dataset = load_dataset(\"open-web-math/open-web-math\")\n",
    "        print(\"Dataset downloaded successfully!\")\n",
    "        \n",
    "        # Print dataset information\n",
    "        print(\"\\nDataset Information:\")\n",
    "        for split_name, split_data in dataset.items():\n",
    "            print(f\"Split: {split_name}, Examples: {len(split_data)}\")\n",
    "            print(f\"Columns: {split_data.column_names}\")\n",
    "            print(f\"First example: {split_data[0]}\")\n",
    "        \n",
    "        # Save the dataset (in parquet format by default for efficiency)\n",
    "        print(\"\\nSaving dataset to disk...\")\n",
    "        save_dataset_to_disk(dataset, output_dir, format=\"parquet\")\n",
    "        \n",
    "        print(\"\\nDataset saved successfully!\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading or saving the dataset: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Memory-efficient script to count samples in a large Parquet file\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "\n",
    "def count_samples(file_path):\n",
    "    \"\"\"\n",
    "    Count samples in a Parquet file using PyArrow (memory efficient)\n",
    "    Also displays file metadata without loading the entire file into memory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            print(f\"Absolute path: {os.path.abspath(file_path)}\")\n",
    "            return None\n",
    "        \n",
    "        # Get file size\n",
    "        file_size_bytes = os.path.getsize(file_path)\n",
    "        file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "        file_size_gb = file_size_mb / 1024\n",
    "        \n",
    "        if file_size_gb >= 1:\n",
    "            print(f\"File size: {file_size_gb:.2f} GB\")\n",
    "        else:\n",
    "            print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Open the file without loading it completely\n",
    "        print(\"Reading Parquet metadata...\")\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        \n",
    "        # Get row count from metadata\n",
    "        count = parquet_file.metadata.num_rows\n",
    "        \n",
    "        # Get schema information\n",
    "        schema = parquet_file.schema\n",
    "        \n",
    "        # Get number of row groups (useful for understanding file structure)\n",
    "        num_row_groups = parquet_file.metadata.num_row_groups\n",
    "        \n",
    "        # Calculate time taken\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nResults for: {os.path.basename(file_path)}\")\n",
    "        print(f\"Total samples: {count:,}\")\n",
    "        print(f\"Number of row groups: {num_row_groups}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        return count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"data/train/train.parquet\"\n",
    "    count_samples(file_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
