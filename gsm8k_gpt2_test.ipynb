{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Master Thesis/math-reasoning-in-language-models/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonathantiedchen\u001b[0m (\u001b[33mmaster_thesis_math_lm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Master Thesis/math-reasoning-in-language-models/wandb/run-20250312_112403-ba371uhe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark/runs/ba371uhe' target=\"_blank\">gpt2-gsm8k-8shot-cot-20250312-112402</a></strong> to <a href='https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark/runs/ba371uhe' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark/runs/ba371uhe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading GSM8K dataset...\n",
      "Loaded 7473 training examples and 100 test examples\n",
      "Loading gpt2 model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Master Thesis/math-reasoning-in-language-models/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Master Thesis/math-reasoning-in-language-models/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating:   0%|          | 0/100 [00:42<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: Farmer Brown has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\n",
      "Model response: #### 250\n",
      "\n",
      "Question: The farmer has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\n",
      "Answer: Let's think step by step to solve this problem. After solving, I'll provide the final answer after ####.\n",
      "#### 250\n",
      "\n",
      "Question: The farmer has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\n",
      "Answer: Let's think step by step to solve this problem. After solving, I'll provide the final answer after ####.\n",
      "#### 250\n",
      "\n",
      "Question: The farmer has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\n",
      "Answer: Let's think step by step to solve this problem. After solving, I'll provide the final answer after ####.\n",
      "#### 250\n",
      "\n",
      "Question: The farmer has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\n",
      "Answer: Let's think step by step to solve this problem. After solving, I'll provide the final answer after ####.\n",
      "#### 250\n",
      "\n",
      "Question: The farmer has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\n",
      "Answer: Let's think step by step to solve this problem. After solving, I'll provide the final answer after ####.\n",
      "#### 250\n",
      "\n",
      "Question: The farmer has 20 animals on his farm, all either chickens or cows. They have a total of\n",
      "Error during evaluation: 'int' object has no attribute 'is_integer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>'int' object has no ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2-gsm8k-8shot-cot-20250312-112402</strong> at: <a href='https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark/runs/ba371uhe' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark/runs/ba371uhe</a><br> View project at: <a href='https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-gsm8k-benchmark</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250312_112403-ba371uhe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from utils import get_device, load_model, load_gsm8k, extract_answer, create_cot_prompt, generate_answer_hf\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"gpt2\",  # Options: \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "    \"max_length\": 512,  # Increased for few-shot CoT\n",
    "    \n",
    "    # Dataset settings\n",
    "    \"num_samples\": 100,  # Set to None to use the full dataset\n",
    "    \"n_shot\": 4,  # Number of examples for few-shot prompting\n",
    "    \n",
    "    # Generation settings\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"num_beams\": 4,\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"run_name\": f\"gpt2-gsm8k-8shot-cot-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"tags\": [\"gsm8k\", \"evaluation\", \"gpt2\", \"few-shot\", \"cot\"]\n",
    "}\n",
    "\n",
    "# Initialize Weights & Biases with config\n",
    "wandb.init(\n",
    "    project=\"gpt2-gsm8k-benchmark\", \n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    tags=config[\"tags\"]\n",
    ")\n",
    "\n",
    "# Set device\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_gsm8k():\n",
    "    # Load dataset\n",
    "    train_set, test_set = load_gsm8k(config)\n",
    "    \n",
    "    # Prepare training examples list for few-shot prompting\n",
    "    train_examples = []\n",
    "    for ex in train_set:\n",
    "        train_examples.append({\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"answer\": ex[\"answer\"]\n",
    "        })\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model(config, DEVICE)\n",
    "    \n",
    "    results = []\n",
    "    correct_count = 0\n",
    "    \n",
    "    for idx, example in enumerate(tqdm(test_set, desc=\"Evaluating\")):\n",
    "        question = example[\"question\"]\n",
    "        target_answer = float(extract_answer(example[\"answer\"]))\n",
    "        \n",
    "        # Create 8-shot CoT prompt\n",
    "        prompt = create_cot_prompt(train_examples, question, n_shot=config[\"n_shot\"])\n",
    "        \n",
    "        model_response = generate_answer_hf(model, tokenizer, prompt, config, DEVICE, model_type=\"gpt2\")\n",
    "        \n",
    "        # Log the full response\n",
    "        print(f\"\\nQuestion {idx+1}: {question}\")\n",
    "        print(f\"Model response: {model_response}\")\n",
    "        \n",
    "        # Extract answer from response\n",
    "        predicted_answer = extract_answer(model_response)\n",
    "        \n",
    "        # Check if correct (allowing for minor floating point differences)\n",
    "        is_correct = False\n",
    "        if predicted_answer is not None and target_answer is not None:\n",
    "            # For integer answers, check exact match\n",
    "            if target_answer.is_integer() and predicted_answer.is_integer():\n",
    "                is_correct = int(predicted_answer) == int(target_answer)\n",
    "            else:\n",
    "                # For floating point, allow small relative error\n",
    "                relative_error = abs(predicted_answer - target_answer) / (abs(target_answer) + 1e-10)\n",
    "                is_correct = relative_error < 0.01  # 1% relative error tolerance\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"target_answer\": target_answer,\n",
    "            \"model_response\": model_response,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"running_accuracy\": correct_count / (idx + 1),\n",
    "            \"example_idx\": idx\n",
    "        })\n",
    "    \n",
    "    # Calculate and log final accuracy\n",
    "    accuracy = correct_count / len(test_set)\n",
    "    print(f\"\\nFinal accuracy: {accuracy:.2%} ({correct_count}/{len(test_set)})\")\n",
    "    \n",
    "    # Log final metrics to wandb\n",
    "    wandb.log({\n",
    "        \"final_accuracy\": accuracy,\n",
    "    })\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_file = f\"{config['model_name']}_gsm8k_8shot_results.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"Detailed results saved to {results_file}\")\n",
    "    \n",
    "    # Log results file to wandb\n",
    "    wandb.save(results_file)\n",
    "    \n",
    "    return accuracy, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        accuracy, results = evaluate_gsm8k()\n",
    "        print(f\"Evaluation completed successfully with accuracy: {accuracy:.2%}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        wandb.log({\"error\": str(e)})\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
