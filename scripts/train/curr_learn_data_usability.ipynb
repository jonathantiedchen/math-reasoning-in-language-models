{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c999a14-b0cc-4727-8524-5c88ae25af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory at: /work/math-reasoning-in-language-models/data\n",
      "✓ Path exists: /work/math-reasoning-in-language-models/data/curriculum_learning/1_ASDiv/ASDiv.xml\n",
      "✓ Path exists: /work/math-reasoning-in-language-models/data/curriculum_learning/2_ParaMAWPS/ParaMAWPS_trainset.json\n",
      "✓ Path exists: /work/math-reasoning-in-language-models/data/curriculum_learning/3_SVAMP/SVAMP.json\n",
      "✓ Path exists: /work/math-reasoning-in-language-models/data/curriculum_learning/4_Dmath/dmath_train.json\n",
      "✓ Path exists: /work/math-reasoning-in-language-models/data/curriculum_learning/5_AQuA/AQuA_train.json\n",
      "\n",
      "\n",
      "Loading ASDiv...\n",
      "Loaded 2305 problems from ASDiv\n",
      "\n",
      "===== SAMPLE FROM ASDiv =====\n",
      "{'text': 'Question: Seven red apples and two green apples are in the basket. How many apples are in the basket?\\nSolution: 7+2=9\\nAnswer: 9 (apples)'}\n",
      "====================================\n",
      "\n",
      "Loading ParaMAWPS...\n",
      "Loaded 13023 problems from ParaMAWPS\n",
      "\n",
      "===== SAMPLE FROM ParaMAWPS =====\n",
      "{'text': 'Question: Bryan took a look at his books as well . If Bryan has 56 books in each of his 9 bookshelves , how many books does he have in total ?\\nEquation: x=56*9\\nAnswer: 504.0'}\n",
      "====================================\n",
      "\n",
      "Loading SVAMP...\n",
      "Loaded 1000 problems from SVAMP\n",
      "\n",
      "===== SAMPLE FROM SVAMP =====\n",
      "{'text': 'Question: Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each pack How much do you have to pay to buy each pack?\\nEquation: ( 76.0 - 25.0 )\\nAnswer: 51.0'}\n",
      "====================================\n",
      "\n",
      "Loading DMath...\n",
      "Loaded 7943 problems from DMath\n",
      "\n",
      "===== SAMPLE FROM DMath =====\n",
      "{'text': 'Question: For the natural number A, the quotient of A divided by 9 is 6 and the remainder is 5. What is the value of A?\\nSolution: var_a = 9\\nvar_b = 6\\nvar_c = var_a * var_b\\nvar_d = 5\\nvar_e = var_c + var_d\\nprint(int(var_e))\\nAnswer: 59'}\n",
      "====================================\n",
      "\n",
      "Loading AQuA...\n",
      "Loaded 97467 problems from AQuA\n",
      "\n",
      "===== SAMPLE FROM AQuA =====\n",
      "{'text': \"Question: Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?\\nOptions:\\nA. A)21\\nB. B)21.5\\nC. C)22\\nD. D)22.5\\nE. E)23\\nRationale: If Q complete x kilometers, then P completes 1.15x kilometers.\\nx + 1.15x = 43\\n2.15x=43\\nx = 43/2.15 = 20\\nThen P will have have walked 1.15*20=23 km.\\nThe answer is E.\\nAnswer: E\"}\n",
      "====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# For Jupyter notebook, we need to set the base directory manually\n",
    "# Assuming you're running this from a notebook in your project\n",
    "BASE_DIR = os.getcwd()\n",
    "# If you're in a subdirectory, you might need to go up a few levels\n",
    "# Uncomment and modify as needed:\n",
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "# Add parent directory to path for importing modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Import data loading functions\n",
    "# Make sure these imports work from your notebook location\n",
    "try:\n",
    "    from utils.data import load_asdiv_data, load_paramawps_data, load_svamp_data, load_aqua_data, load_dmath_data\n",
    "except ImportError:\n",
    "    print(\"Error importing data utilities. Please check your path and make sure you're running this from the correct directory.\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"sys.path: {sys.path}\")\n",
    "    raise\n",
    "\n",
    "# Try to find the correct root directory\n",
    "# Start with BASE_DIR and check a few levels up\n",
    "data_root = None\n",
    "possible_roots = [\n",
    "    BASE_DIR,\n",
    "    os.path.abspath(os.path.join(BASE_DIR, '..')),\n",
    "    os.path.abspath(os.path.join(BASE_DIR, '../..')),\n",
    "    os.path.abspath(os.path.join(BASE_DIR, '../../..'))\n",
    "]\n",
    "\n",
    "for root in possible_roots:\n",
    "    test_path = os.path.join(root, \"data\")\n",
    "    if os.path.exists(test_path):\n",
    "        data_root = root\n",
    "        print(f\"Found data directory at: {test_path}\")\n",
    "        break\n",
    "\n",
    "if data_root is None:\n",
    "    print(\"Could not find data directory. Please specify the path manually.\")\n",
    "    data_root = BASE_DIR  # Default to current directory\n",
    "\n",
    "# Define the data paths relative to data_root\n",
    "data_paths = [\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"1_ASDiv\", \"ASDiv.xml\"),\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"2_ParaMAWPS\", \"ParaMAWPS_trainset.json\"),\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"3_SVAMP\", \"SVAMP.json\"),\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"4_Dmath\", \"dmath_train.json\"),\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"5_AQuA\", \"AQuA_train.json\")  # Added the specific JSON file\n",
    "]\n",
    "\n",
    "# Dataset names for logging\n",
    "dataset_names = [\"ASDiv\", \"ParaMAWPS\", \"SVAMP\", \"DMath\", \"AQuA\"]\n",
    "\n",
    "# Dictionary mapping dataset file paths to their respective loading functions\n",
    "data_loaders = {\n",
    "    data_paths[0]: load_asdiv_data,\n",
    "    data_paths[1]: load_paramawps_data,\n",
    "    data_paths[2]: load_svamp_data,\n",
    "    data_paths[3]: load_dmath_data,\n",
    "    data_paths[4]: load_aqua_data\n",
    "}\n",
    "\n",
    "def print_sample(data, dataset_name):\n",
    "    if data and len(data) > 0:\n",
    "        print(f\"\\n===== SAMPLE FROM {dataset_name} =====\")\n",
    "        print(data[0])  # Simply print the first data item\n",
    "        print(\"====================================\\n\")\n",
    "    else:\n",
    "        print(f\"\\nNo samples available from {dataset_name}\\n\")\n",
    "\n",
    "# Verify paths exist before attempting to load\n",
    "for data_path in data_paths:\n",
    "    if os.path.exists(data_path) or (data_path == data_paths[4] and os.path.isdir(data_path)):\n",
    "        print(f\"✓ Path exists: {data_path}\")\n",
    "    else:\n",
    "        print(f\"✗ Path does not exist: {data_path}\")\n",
    "        \n",
    "print(\"\\n\")\n",
    "\n",
    "# Load and print a sample from each dataset\n",
    "for data_path, dataset_name in zip(data_paths, dataset_names):\n",
    "    print(f\"Loading {dataset_name}...\")\n",
    "    try:\n",
    "        # Get the appropriate loader function for this dataset\n",
    "        loader_func = data_loaders[data_path]\n",
    "        \n",
    "        # Load the dataset\n",
    "        data = loader_func(data_path)\n",
    "        \n",
    "        # Print a sample\n",
    "        print_sample(data, dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f6c45a-aafd-4a5c-adf0-757721a8c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardizing ASDiv...\n",
      "Loaded 2305 problems from ASDiv\n",
      "Formatted 2305 examples from ASDiv\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM ASDiv =====\n",
      "### Question: Seven red apples and two green apples are in the basket. How many apples are in the basket?\n",
      "### Answer: Let me solve this step by step.\n",
      "7+2=9\n",
      "Therefore, the answer is 9 (apples).\n",
      "====================================\n",
      "\n",
      "\n",
      "Standardizing ParaMAWPS...\n",
      "Loaded 13023 problems from ParaMAWPS\n",
      "Formatted 13023 examples from ParaMAWPS\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM ParaMAWPS =====\n",
      "### Question: Bryan took a look at his books as well . If Bryan has 56 books in each of his 9 bookshelves , how many books does he have in total ?\n",
      "### Answer: Let me solve this step by step.\n",
      "x=56*9\n",
      "Therefore, the answer is 504.0.\n",
      "====================================\n",
      "\n",
      "\n",
      "Standardizing SVAMP...\n",
      "Loaded 1000 problems from SVAMP\n",
      "Formatted 1000 examples from SVAMP\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM SVAMP =====\n",
      "### Question: Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each pack How much do you have to pay to buy each pack?\n",
      "### Answer: Let me solve this step by step.\n",
      "( 76.0 - 25.0 )\n",
      "Therefore, the answer is 51.0.\n",
      "====================================\n",
      "\n",
      "\n",
      "Standardizing DMath...\n",
      "Loaded 7943 problems from DMath\n",
      "Formatted 7943 examples from DMath\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM DMath =====\n",
      "### Question: For the natural number A, the quotient of A divided by 9 is 6 and the remainder is 5. What is the value of A?\n",
      "### Answer: Let me solve this step by step.\n",
      "var_a = 9\n",
      "var_b = 6\n",
      "var_c = var_a * var_b\n",
      "var_d = 5\n",
      "var_e = var_c + var_d\n",
      "print(int(var_e))\n",
      "Therefore, the answer is 59.\n",
      "====================================\n",
      "\n",
      "\n",
      "Standardizing AQuA...\n",
      "Loaded 97467 problems from AQuA\n",
      "Formatted 97467 examples from AQuA\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM AQuA =====\n",
      "### Question: Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?\n",
      "Options:\n",
      "A. A)21\n",
      "B. B)21.5\n",
      "C. C)22\n",
      "D. D)22.5\n",
      "E. E)23\n",
      "### Answer: Let me solve this step by step.\n",
      "If Q complete x kilometers, then P completes 1.15x kilometers.\n",
      "x + 1.15x = 43\n",
      "2.15x=43\n",
      "x = 43/2.15 = 20\n",
      "Then P will have have walked 1.15*20=23 km.\n",
      "The answer is E.\n",
      "Therefore, the answer is E.\n",
      "====================================\n",
      "\n",
      "\n",
      "Total combined examples: 121738\n"
     ]
    }
   ],
   "source": [
    "def format_asdiv(item):\n",
    "    \"\"\"Format ASDiv dataset items\"\"\"\n",
    "    text = item['text']\n",
    "    parts = text.split('Question: ')[1].split('\\nSolution:')\n",
    "    question = parts[0].strip()\n",
    "    \n",
    "    solution_answer_parts = parts[1].split('\\nAnswer:')\n",
    "    solution = solution_answer_parts[0].strip()\n",
    "    answer = solution_answer_parts[1].strip()\n",
    "    \n",
    "    formatted_answer = f\"Let me solve this step by step.\\n{solution}\\nTherefore, the answer is {answer}.\"\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': formatted_answer\n",
    "    }\n",
    "\n",
    "def format_paramawps(item):\n",
    "    \"\"\"Format ParaMAWPS dataset items\"\"\"\n",
    "    text = item['text']\n",
    "    parts = text.split('Question: ')[1].split('\\nEquation:')\n",
    "    question = parts[0].strip()\n",
    "    \n",
    "    equation_answer_parts = parts[1].split('\\nAnswer:')\n",
    "    equation = equation_answer_parts[0].strip()\n",
    "    answer = equation_answer_parts[1].strip()\n",
    "    \n",
    "    formatted_answer = f\"Let me solve this step by step.\\n{equation}\\nTherefore, the answer is {answer}.\"\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': formatted_answer\n",
    "    }\n",
    "\n",
    "def format_svamp(item):\n",
    "    \"\"\"Format SVAMP dataset items\"\"\"\n",
    "    text = item['text']\n",
    "    parts = text.split('Question: ')[1].split('\\nEquation:')\n",
    "    question = parts[0].strip()\n",
    "    \n",
    "    equation_answer_parts = parts[1].split('\\nAnswer:')\n",
    "    equation = equation_answer_parts[0].strip()\n",
    "    answer = equation_answer_parts[1].strip()\n",
    "    \n",
    "    formatted_answer = f\"Let me solve this step by step.\\n{equation}\\nTherefore, the answer is {answer}.\"\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': formatted_answer\n",
    "    }\n",
    "\n",
    "def format_dmath(item):\n",
    "    \"\"\"Format DMath dataset items\"\"\"\n",
    "    text = item['text']\n",
    "    parts = text.split('Question: ')[1].split('\\nSolution:')\n",
    "    question = parts[0].strip()\n",
    "    \n",
    "    solution_answer_parts = parts[1].split('\\nAnswer:')\n",
    "    solution = solution_answer_parts[0].strip()\n",
    "    answer = solution_answer_parts[1].strip()\n",
    "    \n",
    "    formatted_answer = f\"Let me solve this step by step.\\n{solution}\\nTherefore, the answer is {answer}.\"\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': formatted_answer\n",
    "    }\n",
    "\n",
    "def format_aqua(item):\n",
    "    \"\"\"Format AQuA dataset items\"\"\"\n",
    "    text = item['text']\n",
    "    \n",
    "    # Extract question and options\n",
    "    if 'Options:' in text:\n",
    "        question_options = text.split('Question: ')[1].split('Rationale:')[0].strip()\n",
    "        \n",
    "        # Split into question and options\n",
    "        question_parts = question_options.split('Options:')\n",
    "        question = question_parts[0].strip()\n",
    "        options = question_parts[1].strip() if len(question_parts) > 1 else \"\"\n",
    "        \n",
    "        # Format the question with options\n",
    "        question = f\"{question}\\nOptions:\\n{options}\"\n",
    "    else:\n",
    "        # Fallback if format is different\n",
    "        question = text.split('Question: ')[1].split('Rationale:')[0].strip()\n",
    "    \n",
    "    # Extract rationale and answer\n",
    "    if 'Rationale:' in text and 'Answer:' in text:\n",
    "        rationale = text.split('Rationale:')[1].split('Answer:')[0].strip()\n",
    "        answer = text.split('Answer:')[1].strip()\n",
    "    else:\n",
    "        # Fallback\n",
    "        rationale = \"\"\n",
    "        answer = text.split('Answer:')[1].strip() if 'Answer:' in text else \"\"\n",
    "    \n",
    "    formatted_answer = f\"Let me solve this step by step.\\n{rationale}\\nTherefore, the answer is {answer}.\"\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': formatted_answer\n",
    "    }\n",
    "\n",
    "# Function to standardize all datasets\n",
    "def standardize_datasets(data_paths, dataset_names, data_loaders):\n",
    "    all_standardized_data = []\n",
    "    \n",
    "    for data_path, dataset_name in zip(data_paths, dataset_names):\n",
    "        try:\n",
    "            print(f\"\\nStandardizing {dataset_name}...\")\n",
    "            \n",
    "            # Load the dataset\n",
    "            loader_func = data_loaders[data_path]\n",
    "            data = loader_func(data_path)\n",
    "            \n",
    "            # Select the appropriate formatting function\n",
    "            if dataset_name == \"ASDiv\":\n",
    "                format_func = format_asdiv\n",
    "            elif dataset_name == \"ParaMAWPS\":\n",
    "                format_func = format_paramawps\n",
    "            elif dataset_name == \"SVAMP\":\n",
    "                format_func = format_svamp\n",
    "            elif dataset_name == \"DMath\":\n",
    "                format_func = format_dmath\n",
    "            elif dataset_name == \"AQuA\":\n",
    "                format_func = format_aqua\n",
    "            else:\n",
    "                print(f\"No formatting function for {dataset_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Format the data\n",
    "            standardized_data = []\n",
    "            for item in data:\n",
    "                try:\n",
    "                    formatted_item = format_func(item)\n",
    "                    standardized_data.append(formatted_item)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error formatting item in {dataset_name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Print an example\n",
    "            if standardized_data:\n",
    "                print(f\"Formatted {len(standardized_data)} examples from {dataset_name}\")\n",
    "                print(f\"\\n===== STANDARDIZED EXAMPLE FROM {dataset_name} =====\")\n",
    "                example = standardized_data[0]\n",
    "                formatted_text = f\"### Question: {example['question']}\\n### Answer: {example['answer']}\"\n",
    "                print(formatted_text)\n",
    "                print(\"====================================\\n\")\n",
    "            \n",
    "            # Add to combined dataset\n",
    "            all_standardized_data.extend(standardized_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\nTotal combined examples: {len(all_standardized_data)}\")\n",
    "    return all_standardized_data\n",
    "\n",
    "# Call the function to process all datasets\n",
    "combined_data = standardize_datasets(data_paths, dataset_names, data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70f984cb-b75a-4509-80a5-777ebc743dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardizing ASDiv...\n",
      "Loaded 2305 problems from ASDiv\n",
      "Formatted 2305 examples from ASDiv\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM ASDiv =====\n",
      "### Question: Seven red apples and two green apples are in the basket. How many apples are in the basket?\n",
      "### Answer: Let me solve this step by step.\n",
      "7+2=9\n",
      "Therefore, the answer is 9 (apples).\n",
      "====================================\n",
      "\n",
      "\n",
      "Standardizing ParaMAWPS...\n",
      "Loaded 13023 problems from ParaMAWPS\n",
      "Formatted 13023 examples from ParaMAWPS\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM ParaMAWPS =====\n",
      "### Question: Bryan took a look at his books as well . If Bryan has 56 books in each of his 9 bookshelves , how many books does he have in total ?\n",
      "### Answer: Let me solve this step by step.\n",
      "x=56*9\n",
      "Therefore, the answer is 504.0.\n",
      "====================================\n",
      "\n",
      "\n",
      "Standardizing SVAMP...\n",
      "Loaded 1000 problems from SVAMP\n",
      "Formatted 1000 examples from SVAMP\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM SVAMP =====\n",
      "### Question: Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each pack How much do you have to pay to buy each pack?\n",
      "### Answer: Let me solve this step by step.\n",
      "( 76.0 - 25.0 )\n",
      "Therefore, the answer is 51.0.\n",
      "====================================\n",
      "\n",
      "\n",
      "Standardizing DMath...\n",
      "Loaded 7943 problems from DMath\n",
      "Formatted 7943 examples from DMath\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM DMath =====\n",
      "### Question: For the natural number A, the quotient of A divided by 9 is 6 and the remainder is 5. What is the value of A?\n",
      "### Answer: Let me solve this step by step.\n",
      "var_a = 9\n",
      "var_b = 6\n",
      "var_c = var_a * var_b\n",
      "var_d = 5\n",
      "var_e = var_c + var_d\n",
      "print(int(var_e))\n",
      "Therefore, the answer is 59.\n",
      "====================================\n",
      "\n",
      "\n",
      "Standardizing AQuA...\n",
      "Loaded 97467 problems from AQuA\n",
      "Formatted 97467 examples from AQuA\n",
      "\n",
      "===== STANDARDIZED EXAMPLE FROM AQuA =====\n",
      "### Question: Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?\n",
      "Options:\n",
      "A. A)21\n",
      "B. B)21.5\n",
      "C. C)22\n",
      "D. D)22.5\n",
      "E. E)23\n",
      "### Answer: Let me solve this step by step.\n",
      "If Q complete x kilometers, then P completes 1.15x kilometers.\n",
      "x + 1.15x = 43\n",
      "2.15x=43\n",
      "x = 43/2.15 = 20\n",
      "Then P will have have walked 1.15*20=23 km.\n",
      "The answer is E.\n",
      "Therefore, the answer is E.\n",
      "====================================\n",
      "\n",
      "\n",
      "Dataset statistics:\n",
      "ASDiv: 2305 examples\n",
      "ParaMAWPS: 13023 examples\n",
      "SVAMP: 1000 examples\n",
      "DMath: 7943 examples\n",
      "AQuA: 97467 examples\n",
      "\n",
      "Total examples across all datasets: 121738\n"
     ]
    }
   ],
   "source": [
    "# Function to standardize all datasets but keep them separate\n",
    "def standardize_datasets(data_paths, dataset_names, data_loaders):\n",
    "    # Create a dictionary to store each dataset separately\n",
    "    standardized_datasets = {}\n",
    "    \n",
    "    for data_path, dataset_name in zip(data_paths, dataset_names):\n",
    "        try:\n",
    "            print(f\"\\nStandardizing {dataset_name}...\")\n",
    "            \n",
    "            # Load the dataset\n",
    "            loader_func = data_loaders[data_path]\n",
    "            data = loader_func(data_path)\n",
    "            \n",
    "            # Select the appropriate formatting function\n",
    "            if dataset_name == \"ASDiv\":\n",
    "                format_func = format_asdiv\n",
    "            elif dataset_name == \"ParaMAWPS\":\n",
    "                format_func = format_paramawps\n",
    "            elif dataset_name == \"SVAMP\":\n",
    "                format_func = format_svamp\n",
    "            elif dataset_name == \"DMath\":\n",
    "                format_func = format_dmath\n",
    "            elif dataset_name == \"AQuA\":\n",
    "                format_func = format_aqua\n",
    "            else:\n",
    "                print(f\"No formatting function for {dataset_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Format the data\n",
    "            standardized_data = []\n",
    "            for item in data:\n",
    "                try:\n",
    "                    formatted_item = format_func(item)\n",
    "                    standardized_data.append(formatted_item)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error formatting item in {dataset_name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Print an example\n",
    "            if standardized_data:\n",
    "                print(f\"Formatted {len(standardized_data)} examples from {dataset_name}\")\n",
    "                print(f\"\\n===== STANDARDIZED EXAMPLE FROM {dataset_name} =====\")\n",
    "                example = standardized_data[0]\n",
    "                formatted_text = f\"### Question: {example['question']}\\n### Answer: {example['answer']}\"\n",
    "                print(formatted_text)\n",
    "                print(\"====================================\\n\")\n",
    "            \n",
    "            # Store dataset separately in the dictionary\n",
    "            standardized_datasets[dataset_name] = standardized_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Print stats for each dataset\n",
    "    print(\"\\nDataset statistics:\")\n",
    "    total_examples = 0\n",
    "    for name, dataset in standardized_datasets.items():\n",
    "        print(f\"{name}: {len(dataset)} examples\")\n",
    "        total_examples += len(dataset)\n",
    "    \n",
    "    print(f\"\\nTotal examples across all datasets: {total_examples}\")\n",
    "    \n",
    "    return standardized_datasets\n",
    "\n",
    "# Call the function to process all datasets\n",
    "dataset_dict = standardize_datasets(data_paths, dataset_names, data_loaders)\n",
    "\n",
    "# Now you have access to each dataset separately:\n",
    "asdiv_data = dataset_dict[\"ASDiv\"]\n",
    "paramawps_data = dataset_dict[\"ParaMAWPS\"]\n",
    "svamp_data = dataset_dict[\"SVAMP\"]\n",
    "dmath_data = dataset_dict[\"DMath\"]\n",
    "aqua_data = dataset_dict[\"AQuA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0738da6-67b9-4990-a519-69d92482afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import sys\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b9ec2-e3cd-4dcc-8b33-abe7a15f8e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:98wnz1cw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>model_total_params</td><td>▁</td></tr><tr><td>model_trainable_params</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>model_total_params</td><td>124439808</td></tr><tr><td>model_trainable_params</td><td>124439808</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2-math-curriculum-sample-sft</strong> at: <a href='https://wandb.ai/master_thesis_math_lm/curriculum_learning_math/runs/98wnz1cw' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/curriculum_learning_math/runs/98wnz1cw</a><br/> View project at: <a href='https://wandb.ai/master_thesis_math_lm/curriculum_learning_math' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/curriculum_learning_math</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250318_150442-98wnz1cw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:98wnz1cw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/math-reasoning-in-language-models/scripts/train/wandb/run-20250318_163027-k5npu7p1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/k5npu7p1' target=\"_blank\">curriculum-learning-sft</a></strong> to <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-math</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/k5npu7p1' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/k5npu7p1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact gpt2-math-model:v0, 479.31MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:1.2\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"gpt2-math\", name=\"curriculum-learning-sft\")\n",
    "artifact = run.use_artifact('master_thesis_math_lm/gpt2-math/gpt2-math-model:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eafb12ca-b5e3-4f8e-8bf0-345790b4f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a1f7663-c2c7-4c86-b4aa-5c62be8e5423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d7b8764c-dd38-471d-a55b-8772222da890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Seven red apples and two green apples are in the basket. How many apples are in the basket?',\n",
       "  'output': 'Let me solve this step by step.\\n7+2=9\\nTherefore, the answer is 9 (apples).'},\n",
       " {'instruction': 'Ellen has six more balls than Marin. Marin has nine balls. How many balls does Ellen have?',\n",
       "  'output': 'Let me solve this step by step.\\n6+9=15\\nTherefore, the answer is 15 (balls).'},\n",
       " {'instruction': 'Janet has nine oranges and Sharon has seven oranges. How many oranges do Janet and Sharon have together?',\n",
       "  'output': 'Let me solve this step by step.\\n9+7=16\\nTherefore, the answer is 16 (oranges).'},\n",
       " {'instruction': 'Allan brought two balloons and Jake brought four balloons to the park. How many balloons did Allan and Jake have in the park?',\n",
       "  'output': 'Let me solve this step by step.\\n2+4=6\\nTherefore, the answer is 6 (balloons).'},\n",
       " {'instruction': 'Adam has five more apples than Jackie. Jackie has nine apples. How many apples does Adam have?',\n",
       "  'output': 'Let me solve this step by step.\\n5+9=14\\nTherefore, the answer is 14 (apples).'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_prompts_func_old(examples):\n",
    "    output_texts = []\n",
    "    for example in examples:  # Limit to 5 samples\n",
    "        text = f\"### Question: {example['question']}\\n### Answer: {example['answer']}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    formatted_data = []\n",
    "    for example in examples[:5]:  # Limit to 5 samples\n",
    "        formatted_data.append({\n",
    "            \"instruction\": example[\"question\"],\n",
    "            \"output\": example[\"answer\"]\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "asdiv_formatted = formatting_prompts_func(asdiv_data)\n",
    "paramawps_formatted = formatting_prompts_func(paramawps_data)\n",
    "svamp_formatted = formatting_prompts_func(svamp_data)\n",
    "dmath_formatted = formatting_prompts_func(dmath_data)\n",
    "aqua_formatted = formatting_prompts_func(aqua_data)\n",
    "\n",
    "asdiv_small = asdiv_formatted[:5]\n",
    "asdiv_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cff0606b-dc84-4ce1-be39-f9eb83ffc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(asdiv_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d7041a1-c7b7-43fd-8100-e68068b37492",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3e0234ab-bdde-4d47-b207-7af4bb7c5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(example):\n",
    "    example['prompt'] = f\"\"\"<|system|>\n",
    "    You are a intelligent chatbot and expertise in Mathematics.</s>\n",
    "    <|user|>\n",
    "    {example['instruction']}.\n",
    "    <|assistant|>\n",
    "    {example['output']}\"\"\"\n",
    "    return example\n",
    "\n",
    "def tokenize_datasets(dataset):\n",
    "    tokenized_dataset = dataset.map(\n",
    "      lambda example: tokenizer(\n",
    "          example['prompt'],\n",
    "          truncation=True,\n",
    "          max_length=512,\n",
    "          ),\n",
    "      batched=True,\n",
    "      remove_columns=['prompt'])\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1dd4070f-158a-474b-a6ed-32e820f8ca3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353d6c2b9ee04a78b5c03accc8ee5df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    prepare_datasets, remove_columns=['instruction', 'output']\n",
    ")\n",
    "#dataset = dataset.shuffle(42).select(range(395000)).train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "90008149-9363-48ee-89af-060187f7146a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d723c3b7d3a3470485ac0b1d688913e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2342d1bb1c7491a94eeffdc0c9e2429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a35e73116e249da83de2a24329c36b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f0a3bda6a04982a9a6f4d727e9d615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "batch_size = 2\n",
    "max_steps = 100\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=SFTConfig(\n",
    "        output_dir=\"./models/mathgpt2sft/\",\n",
    "        gradient_accumulation_steps=batch_size,\n",
    "        #evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"no\",\n",
    "        save_total_limit=2,\n",
    "        save_safetensors=False,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "        eval_steps=50,\n",
    "        max_steps=max_steps,\n",
    "        warmup_steps=30,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        lr_scheduler_type=\"cosine\"\n",
    "    ),\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0199300-4cda-4ae4-923e-31953fe4864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 2\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: prompt. If prompt are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 5\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 124,439,808\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/100 00:05 < 08:42, 0.19 it/s, Epoch 1/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3738\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/accelerate/accelerator.py:2359\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2359\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc1b75-9855-4175-96a3-c524babc3d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
