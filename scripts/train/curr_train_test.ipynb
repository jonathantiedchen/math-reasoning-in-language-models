{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2212a8dd-ed5f-456c-acac-8f5dc6d953e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9nqt1255) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">curriculum-learning-sft</strong> at: <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/9nqt1255' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/9nqt1255</a><br/> View project at: <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-math</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250318_152310-9nqt1255/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9nqt1255). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/math-reasoning-in-language-models/scripts/train/wandb/run-20250318_152433-fghnyq1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/fghnyq1y' target=\"_blank\">curriculum-learning-sft</a></strong> to <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-math</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/fghnyq1y' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/fghnyq1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact gpt2-math-model:v0, 479.31MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating data directory...\n",
      "Found data directory at: /work/math-reasoning-in-language-models/data\n",
      "✓ Found ASDiv dataset at: /work/math-reasoning-in-language-models/data/curriculum_learning/1_ASDiv/ASDiv.xml\n",
      "✓ Found ParaMAWPS dataset at: /work/math-reasoning-in-language-models/data/curriculum_learning/2_ParaMAWPS/ParaMAWPS_trainset.json\n",
      "✓ Found SVAMP dataset at: /work/math-reasoning-in-language-models/data/curriculum_learning/3_SVAMP/SVAMP.json\n",
      "✓ Found DMath dataset at: /work/math-reasoning-in-language-models/data/curriculum_learning/4_Dmath/dmath_train.json\n",
      "✓ Found AQuA dataset at: /work/math-reasoning-in-language-models/data/curriculum_learning/5_AQuA/AQuA_train.json\n",
      "\n",
      "Starting curriculum learning...\n",
      "\n",
      "==================================================\n",
      "Training on ASDiv dataset\n",
      "==================================================\n",
      "Loading data from /work/math-reasoning-in-language-models/data/curriculum_learning/1_ASDiv/ASDiv.xml\n",
      "Loaded 2305 problems from ASDiv\n",
      "Loaded 5 samples\n",
      "Created dataset object\n",
      "Tokenized the data\n",
      "Error training on ASDiv dataset: 'str' object has no attribute 'keys'\n",
      "\n",
      "==================================================\n",
      "Training on ParaMAWPS dataset\n",
      "==================================================\n",
      "Loading data from /work/math-reasoning-in-language-models/data/curriculum_learning/2_ParaMAWPS/ParaMAWPS_trainset.json\n",
      "Loaded 13023 problems from ParaMAWPS\n",
      "Loaded 5 samples\n",
      "Created dataset object\n",
      "Tokenized the data\n",
      "Error training on ParaMAWPS dataset: 'str' object has no attribute 'keys'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2051/664185450.py:123: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "Training on SVAMP dataset\n",
      "==================================================\n",
      "Loading data from /work/math-reasoning-in-language-models/data/curriculum_learning/3_SVAMP/SVAMP.json\n",
      "Loaded 1000 problems from SVAMP\n",
      "Loaded 5 samples\n",
      "Created dataset object\n",
      "Tokenized the data\n",
      "Error training on SVAMP dataset: 'str' object has no attribute 'keys'\n",
      "\n",
      "==================================================\n",
      "Training on DMath dataset\n",
      "==================================================\n",
      "Loading data from /work/math-reasoning-in-language-models/data/curriculum_learning/4_Dmath/dmath_train.json\n",
      "Loaded 7943 problems from DMath\n",
      "Loaded 5 samples\n",
      "Created dataset object\n",
      "Tokenized the data\n",
      "Error training on DMath dataset: 'str' object has no attribute 'keys'\n",
      "\n",
      "==================================================\n",
      "Training on AQuA dataset\n",
      "==================================================\n",
      "Loading data from /work/math-reasoning-in-language-models/data/curriculum_learning/5_AQuA/AQuA_train.json\n",
      "Loaded 97467 problems from AQuA\n",
      "Loaded 5 samples\n",
      "Created dataset object\n",
      "Tokenized the data\n",
      "Error training on AQuA dataset: 'str' object has no attribute 'keys'\n",
      "\n",
      "Curriculum learning complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">curriculum-learning-sft</strong> at: <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/fghnyq1y' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-math/runs/fghnyq1y</a><br/> View project at: <a href='https://wandb.ai/master_thesis_math_lm/gpt2-math' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/gpt2-math</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250318_152433-fghnyq1y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import sys\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from utils.data import load_asdiv_data, load_paramawps_data, load_svamp_data, load_aqua_data, load_dmath_data\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(project=\"gpt2-math\", name=\"curriculum-learning-sft\")\n",
    "artifact = run.use_artifact('master_thesis_math_lm/gpt2-math/gpt2-math-model:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(artifact_dir)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "\n",
    "# Define dataset paths\n",
    "BASE_DIR = os.getcwd()\n",
    "# Your existing code for finding the data directory\n",
    "print(\"Locating data directory...\")\n",
    "data_root = None\n",
    "possible_roots = [\n",
    "    BASE_DIR,\n",
    "    os.path.abspath(os.path.join(BASE_DIR, '..')),\n",
    "    os.path.abspath(os.path.join(BASE_DIR, '../..')),\n",
    "    os.path.abspath(os.path.join(BASE_DIR, '../../..'))\n",
    "]\n",
    "for root in possible_roots:\n",
    "    test_path = os.path.join(root, \"data\")\n",
    "    if os.path.exists(test_path):\n",
    "        data_root = root\n",
    "        print(f\"Found data directory at: {test_path}\")\n",
    "        break\n",
    "if data_root is None:\n",
    "    print(\"Could not find data directory. Please specify the path manually.\")\n",
    "    data_root = BASE_DIR  # Default to current directory\n",
    "\n",
    "# Define the data paths relative to data_root\n",
    "data_paths = [\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"1_ASDiv\", \"ASDiv.xml\"),\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"2_ParaMAWPS\", \"ParaMAWPS_trainset.json\"),\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"3_SVAMP\", \"SVAMP.json\"),\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"4_Dmath\", \"dmath_train.json\"),\n",
    "    os.path.join(data_root, \"data\", \"curriculum_learning\", \"5_AQuA\", \"AQuA_train.json\")\n",
    "]\n",
    "\n",
    "# Dataset names for logging\n",
    "dataset_names = [\"ASDiv\", \"ParaMAWPS\", \"SVAMP\", \"DMath\", \"AQuA\"]\n",
    "\n",
    "# Verify data files exist\n",
    "for path, name in zip(data_paths, dataset_names):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"✓ Found {name} dataset at: {path}\")\n",
    "    else:\n",
    "        print(f\"✗ Could not find {name} dataset at: {path}\")\n",
    "\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "    return tokenizer(\n",
    "        [sample['text'] for sample in data],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Dictionary mapping dataset file paths to their respective loading functions\n",
    "data_loaders = {\n",
    "    data_paths[0]: load_asdiv_data,\n",
    "    data_paths[1]: load_paramawps_data,\n",
    "    data_paths[2]: load_svamp_data,\n",
    "    data_paths[3]: load_dmath_data,\n",
    "    data_paths[4]: load_aqua_data\n",
    "}\n",
    "\n",
    "\n",
    "# Training loop over datasets\n",
    "print(\"\\nStarting curriculum learning...\")\n",
    "for path, name in zip(data_paths, dataset_names):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training on {name} dataset\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Skipping {name} - file not found\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading data from {path}\")\n",
    "        # Pass the file path to the loader function\n",
    "        raw_data = data_loaders[path](path)[:5]  # Select only 5 samples\n",
    "        print(f\"Loaded {len(raw_data)} samples\")\n",
    "        \n",
    "        dataset = Dataset.from_list(raw_data)\n",
    "        print(\"Created dataset object\")\n",
    "        \n",
    "        tokenized_data = preprocess_data(dataset, tokenizer)\n",
    "        print(\"Tokenized the data\")\n",
    "        \n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./models/gpt2-math-curriculum/{name}\",\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=1,\n",
    "            save_steps=10,\n",
    "            logging_steps=1,\n",
    "            num_train_epochs=1,\n",
    "            report_to=[\"wandb\"],\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=tokenized_data,\n",
    "            args=training_args,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training on {name} dataset...\")\n",
    "        trainer.train()\n",
    "        print(f\"Completed training on {name} dataset\")\n",
    "        \n",
    "        # Save the model for this dataset\n",
    "        model_save_path = f\"./models/gpt2-math-curriculum/{name}\"\n",
    "        model.save_pretrained(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        print(f\"Saved model to {model_save_path}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({f\"trained_on_{name}\": True})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training on {name} dataset: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nCurriculum learning complete!\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5ee13-31ce-4baa-a460-c5e26de43f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
