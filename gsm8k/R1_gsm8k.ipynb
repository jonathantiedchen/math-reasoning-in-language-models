{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecfb53f-1b2e-41c8-b6f4-b6154dc4c961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8K dataset downloaded successfully.\n",
      "Loaded 7473 training examples and 1319 test examples\n",
      "Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\n",
      "Reference answer: The discount price of one glass is 60/100 * 5 = $<<60/100*5=3>>3.\n",
      "If every second glass is cheaper, that means Kylar is going to buy 16 / 2 = <<16/2=8>>8 cheaper glasses.\n",
      "So for the cheaper glasses, Kylar is going to pay 8 * 3 = $<<8*3=24>>24.\n",
      "And for the regular-priced glasses, Kylar will pay 8 * 5 = $<<8*5=40>>40.\n",
      "So in total Kylar needs to pay 24 + 40 = $<<24+40=64>>64 for the glasses he wants to buy.\n",
      "#### 64\n",
      "\n",
      "DeepSeek Solution:\n",
      "'\n",
      "</think>\n",
      "\n",
      "Kylar wants to buy 16 glasses, each costing $5, but every second glass costs only 60% of the price. Let's break down the cost step by step.\n",
      "\n",
      "1. **First 8 glasses:**\n",
      "   - Each glass costs $5.\n",
      "   - Total cost for 8 glasses: \\(8 \\times 5 = \\$40\\).\n",
      "\n",
      "2. **Next 8 glasses (every second glass):**\n",
      "   - Each glass costs 60% of $5, which is \\(0.6 \\times 5 = \\$3\\).\n",
      "   - Total cost for 8 glasses: \\(8 \\times 3 = \\$24\\).\n",
      "\n",
      "3. **Total cost for 16 glasses:**\n",
      "   - Sum of the costs for the first 8 and the next 8 glasses: \\(40 + 24 = \\$64\\).\n",
      "\n",
      "\\boxed{64}\n",
      "\n",
      "Extracted answer: 64.0\n",
      "Reference numeric answer: 64.0\n",
      "✓ Correct!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "# Make sure padding token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Function to download GSM8K dataset from GitHub\n",
    "def download_gsm8k():\n",
    "    import requests\n",
    "    import os\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(\"gsm8k_data\", exist_ok=True)\n",
    "    \n",
    "    # URLs for train and test data\n",
    "    train_url = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/train.jsonl\"\n",
    "    test_url = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl\"\n",
    "    \n",
    "    # Download train set\n",
    "    train_response = requests.get(train_url)\n",
    "    with open(\"gsm8k_data/train.jsonl\", \"wb\") as f:\n",
    "        f.write(train_response.content)\n",
    "    \n",
    "    # Download test set\n",
    "    test_response = requests.get(test_url)\n",
    "    with open(\"gsm8k_data/test.jsonl\", \"wb\") as f:\n",
    "        f.write(test_response.content)\n",
    "    \n",
    "    print(\"GSM8K dataset downloaded successfully.\")\n",
    "\n",
    "# Function to load the GSM8K dataset from files\n",
    "def load_gsm8k_from_file():\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    # Check if files exist, download if not\n",
    "    if not (os.path.exists(\"gsm8k_data/train.jsonl\") and os.path.exists(\"gsm8k_data/test.jsonl\")):\n",
    "        download_gsm8k()\n",
    "    \n",
    "    # Load train data\n",
    "    with open(\"gsm8k_data/train.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            train_data.append(json.loads(line))\n",
    "    \n",
    "    # Load test data\n",
    "    with open(\"gsm8k_data/test.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            test_data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Loaded {len(train_data)} training examples and {len(test_data)} test examples\")\n",
    "    return train_data, test_data\n",
    "\n",
    "# Load our GSM8K dataset\n",
    "train_set, test_set = load_gsm8k_from_file()\n",
    "\n",
    "# Function to extract the numerical answer\n",
    "def extract_answer(text):\n",
    "    if \"####\" in text:\n",
    "        answer_part = text.split(\"####\")[-1].strip()\n",
    "        numbers = re.findall(r'[-+]?\\d*\\.?\\d+', answer_part)\n",
    "        if numbers:\n",
    "            return float(numbers[0])\n",
    "    \n",
    "    # Fallback: extract the last number in the full text\n",
    "    numbers = re.findall(r'[-+]?\\d*\\.?\\d+', text)\n",
    "    if numbers:\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    return None\n",
    "\n",
    "# For demonstration, let's use a sample from GSM8K\n",
    "sample_idx = 5  # You can change this to try different examples\n",
    "sample = test_set[sample_idx]\n",
    "question = sample[\"question\"]\n",
    "reference_answer = sample[\"answer\"]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Reference answer: {reference_answer}\")\n",
    "\n",
    "# Create the prompt for DeepSeek model\n",
    "# The DeepSeek models work well with explicit chain-of-thought prompting\n",
    "prompt = f\"{question}Let's solve it step by step. Make sure to stop immediately after writing the final answer. You MUST write the final answer as an integer after '####'.\"\n",
    "\n",
    "# Generate answer\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(generated_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nDeepSeek Solution:\")\n",
    "print(response)\n",
    "\n",
    "# Extract the numerical answer for evaluation\n",
    "predicted_answer = extract_answer(response)\n",
    "reference_numeric = extract_answer(reference_answer)\n",
    "\n",
    "print(f\"\\nExtracted answer: {predicted_answer}\")\n",
    "print(f\"Reference numeric answer: {reference_numeric}\")\n",
    "\n",
    "# Check if the answer is correct\n",
    "if predicted_answer is not None and reference_numeric is not None:\n",
    "    # Allow for small floating point differences\n",
    "    if abs(predicted_answer - reference_numeric) < 1e-6:\n",
    "        print(\"✓ Correct!\")\n",
    "    else:\n",
    "        print(\"✗ Incorrect\")\n",
    "else:\n",
    "    print(\"Could not extract a numeric answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532a170-8d94-40ce-8cc1-9cccd12e3ecf",
   "metadata": {},
   "source": [
    "### Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbc7227-d15f-407b-9360-a5d71c4f3ebd",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataFilesNotFoundError",
     "evalue": "No (supported) data files found in gsm8k",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataFilesNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load GSM8K dataset\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m gsm8k \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgsm8k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_set \u001b[38;5;241m=\u001b[39m gsm8k[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m train_set \u001b[38;5;241m=\u001b[39m gsm8k[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/load.py:1849\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1848\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1849\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/load.py:1586\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LocalDatasetModuleFactoryWithScript(\n\u001b[1;32m   1578\u001b[0m         combined_path,\n\u001b[1;32m   1579\u001b[0m         download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1580\u001b[0m         dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1581\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   1582\u001b[0m     )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[0;32m-> 1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;66;03m# Try remotely\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_relative_path(path) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/load.py:842\u001b[0m, in \u001b[0;36mLocalDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m get_data_patterns(base_path)\n\u001b[1;32m    837\u001b[0m data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_patterns(\n\u001b[1;32m    838\u001b[0m     patterns,\n\u001b[1;32m    839\u001b[0m     base_path\u001b[38;5;241m=\u001b[39mbase_path,\n\u001b[1;32m    840\u001b[0m     allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    841\u001b[0m )\n\u001b[0;32m--> 842\u001b[0m module_name, default_builder_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43minfer_module_for_data_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m data_files \u001b[38;5;241m=\u001b[39m data_files\u001b[38;5;241m.\u001b[39mfilter_extensions(_MODULE_TO_EXTENSIONS[module_name])\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# Collect metadata files if the module supports them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/load.py:603\u001b[0m, in \u001b[0;36minfer_module_for_data_files\u001b[0;34m(data_files, path, download_config)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt infer the same data file format for all splits. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name:\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataFilesNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo (supported) data files found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module_name, default_builder_kwargs\n",
      "\u001b[0;31mDataFilesNotFoundError\u001b[0m: No (supported) data files found in gsm8k"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "# Make sure padding token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "test_set = gsm8k[\"test\"]\n",
    "train_set = gsm8k[\"train\"]\n",
    "\n",
    "# Function to extract the numerical answer\n",
    "def extract_answer(text):\n",
    "    if \"####\" in text:\n",
    "        answer_part = text.split(\"####\")[-1].strip()\n",
    "        numbers = re.findall(r'[-+]?\\d*\\.?\\d+', answer_part)\n",
    "        if numbers:\n",
    "            return float(numbers[0])\n",
    "    \n",
    "    # Fallback: extract the last number in the full text\n",
    "    numbers = re.findall(r'[-+]?\\d*\\.?\\d+', text)\n",
    "    if numbers:\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to create 5-shot prompt\n",
    "def create_five_shot_prompt(train_examples, question):\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Sample 5 examples from the training set\n",
    "    shot_examples = random.sample(train_examples, 5)\n",
    "    \n",
    "    # Build the few-shot prompt\n",
    "    prompt = \"\"\n",
    "    for ex in shot_examples:\n",
    "        prompt += f\"Question: {ex['question']}\\n\"\n",
    "        prompt += f\"Answer: {ex['answer']}\\n\\n\"\n",
    "    \n",
    "    # Add the current question\n",
    "    prompt += f\"Question: {question} Let's solve it step by step. Please stop after you provided the final answer. You MUST provide the final answer as an integer after '####'.\\n\"\n",
    "\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# For demonstration, let's use a sample from GSM8K\n",
    "sample_idx = 5  # You can change this to try different examples\n",
    "sample = test_set[sample_idx]\n",
    "question = sample[\"question\"]\n",
    "reference_answer = sample[\"answer\"]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Reference answer: {reference_answer}\")\n",
    "\n",
    "# Create 5-shot prompt\n",
    "five_shot_prompt = create_five_shot_prompt(list(train_set), question)\n",
    "\n",
    "print(\"\\n5-Shot Prompt (excerpt):\")\n",
    "# Print the beginning and end of the prompt if it's long\n",
    "if len(five_shot_prompt) > 500:\n",
    "    print(five_shot_prompt[:250] + \"...\\n...\" + five_shot_prompt[-250:])\n",
    "else:\n",
    "    print(five_shot_prompt)\n",
    "\n",
    "# Generate answer\n",
    "inputs = tokenizer(five_shot_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(generated_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nDeepSeek Solution:\")\n",
    "print(response)\n",
    "\n",
    "# Extract the numerical answer for evaluation\n",
    "predicted_answer = extract_answer(response)\n",
    "reference_numeric = extract_answer(reference_answer)\n",
    "\n",
    "print(f\"\\nExtracted answer: {predicted_answer}\")\n",
    "print(f\"Reference numeric answer: {reference_numeric}\")\n",
    "\n",
    "# Check if the answer is correct\n",
    "if predicted_answer is not None and reference_numeric is not None:\n",
    "    # Allow for small floating point differences\n",
    "    if abs(predicted_answer - reference_numeric) < 1e-6:\n",
    "        print(\"✓ Correct!\")\n",
    "    else:\n",
    "        print(\"✗ Incorrect\")\n",
    "else:\n",
    "    print(\"Could not extract a numeric answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb79bb-b799-46b8-b25e-34d99bd8b86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
