{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Master Thesis/math-reasoning-in-language-models/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonathantiedchen\u001b[0m (\u001b[33mmaster_thesis_math_lm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Master Thesis/math-reasoning-in-language-models/wandb/run-20250312_102430-gbxugmw3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark/runs/gbxugmw3' target=\"_blank\">deepseek-gsm8k-8shot-20250312-102429</a></strong> to <a href='https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark/runs/gbxugmw3' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark/runs/gbxugmw3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading GSM8K dataset...\n",
      "Loaded 7473 training examples and 100 test examples\n",
      "Loading deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Evaluating:   0%|          | 0/100 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: MPS backend out of memory (MPS allocated: 8.82 GB, other allocations: 9.15 GB, max allowed: 18.13 GB). Tried to allocate 229.28 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error</td><td>MPS backend out of m...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deepseek-gsm8k-8shot-20250312-102429</strong> at: <a href='https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark/runs/gbxugmw3' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark/runs/gbxugmw3</a><br> View project at: <a href='https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark' target=\"_blank\">https://wandb.ai/master_thesis_math_lm/deepseek-gsm8k-benchmark</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250312_102430-gbxugmw3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"max_length\": 512,  # Increased to handle longer few-shot prompts\n",
    "    \n",
    "    # Dataset settings\n",
    "    \"num_samples\": 100,  # Set to None to use the full dataset\n",
    "    \"n_shot\": 8,  # Number of examples for few-shot prompting\n",
    "    \n",
    "    # Generation settings\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"num_beams\": 4,\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"run_name\": f\"deepseek-gsm8k-8shot-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"tags\": [\"gsm8k\", \"evaluation\", \"deepseek\", \"qwen\", \"few-shot\"]\n",
    "}\n",
    "\n",
    "# Initialize Weights & Biases with config\n",
    "wandb.init(\n",
    "    project=\"deepseek-gsm8k-benchmark\", \n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    tags=config[\"tags\"]\n",
    ")\n",
    "\n",
    "# Set device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load GSM8K dataset\n",
    "def load_gsm8k():\n",
    "    print(\"Loading GSM8K dataset...\")\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "    \n",
    "    # We'll use examples from train split for few-shot demonstrations\n",
    "    train_set = dataset[\"train\"]\n",
    "    test_set = dataset[\"test\"]\n",
    "    \n",
    "    if config[\"num_samples\"]:\n",
    "        # Randomly sample a subset if num_samples is specified\n",
    "        indices = np.random.choice(len(test_set), min(config[\"num_samples\"], len(test_set)), replace=False)\n",
    "        test_set = test_set.select(indices)\n",
    "    \n",
    "    print(f\"Loaded {len(train_set)} training examples and {len(test_set)} test examples\")\n",
    "    return train_set, test_set\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_model():\n",
    "    print(f\"Loading {config['model_name']} model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"], use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(config[\"model_name\"])\n",
    "    \n",
    "    # Handle padding token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Extract answer from model's response\n",
    "def extract_answer(text):\n",
    "    # First, try to extract the answer after #### delimiter\n",
    "    if \"####\" in text:\n",
    "        answer_part = text.split(\"####\")[-1].strip()\n",
    "        \n",
    "        # Remove thousand separators and units\n",
    "        answer_part = re.sub(r'[$,]', '', answer_part)\n",
    "        answer_part = re.sub(r'(\\d+)\\s*(?:dollars|units|kg|m|cm|km|mph|lbs|inches|feet|tons|hours|minutes)', r'\\1', answer_part)\n",
    "        \n",
    "        # Extract the numerical answer\n",
    "        numbers = re.findall(r'[-+]?\\d*\\.?\\d+', answer_part)\n",
    "        if numbers:\n",
    "            return float(numbers[0])  # Take the first number after ####\n",
    "    \n",
    "    # Fallback: extract the last number in the full text\n",
    "    # Remove thousand separators and units first\n",
    "    cleaned_text = re.sub(r'[$,]', '', text)\n",
    "    cleaned_text = re.sub(r'(\\d+)\\s*(?:dollars|units|kg|m|cm|km|mph|lbs|inches|feet|tons|hours|minutes)', r'\\1', cleaned_text)\n",
    "    numbers = re.findall(r'[-+]?\\d*\\.?\\d+', cleaned_text)\n",
    "    if numbers:\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    # If no number found\n",
    "    return None\n",
    "\n",
    "# Create 8-shot prompt with examples from the training set\n",
    "def create_few_shot_prompt(train_examples, question, n_shot=8):\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Sample n examples from the training set\n",
    "    shot_examples = random.sample(train_examples, n_shot)\n",
    "    \n",
    "    # Build the few-shot prompt\n",
    "    prompt = \"\"\n",
    "    for ex in shot_examples:\n",
    "        prompt += f\"Question: {ex['question']}\\n\"\n",
    "        prompt += f\"Answer: {ex['answer']}\\n\\n\"\n",
    "    \n",
    "    # Add the current question and instruction\n",
    "    prompt += f\"Question: {question}\\n\"\n",
    "    prompt += f\"Answer: Let's think step by step. At the end, I'll write the answer as a number after '####'.\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Generate answer with DeepSeek model using 8-shot prompting\n",
    "def generate_answer(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=len(inputs[\"input_ids\"][0]) + config[\"max_length\"],\n",
    "            temperature=config[\"temperature\"],\n",
    "            top_p=config[\"top_p\"],\n",
    "            num_beams=config[\"num_beams\"],\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated response (after the last \"Answer: \")\n",
    "    try:\n",
    "        generated_text = response.split(\"Answer: \")[-1].strip()\n",
    "    except:\n",
    "        generated_text = response\n",
    "        \n",
    "    return generated_text\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_gsm8k_few_shot():\n",
    "    # Load dataset\n",
    "    train_set, test_set = load_gsm8k()\n",
    "    \n",
    "    # Prepare training examples list for few-shot prompting\n",
    "    train_examples = []\n",
    "    for ex in train_set:\n",
    "        train_examples.append({\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"answer\": ex[\"answer\"]\n",
    "        })\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "    \n",
    "    results = []\n",
    "    correct_count = 0\n",
    "    \n",
    "    for idx, example in enumerate(tqdm(test_set, desc=\"Evaluating\")):\n",
    "        question = example[\"question\"]\n",
    "        target_answer = float(extract_answer(example[\"answer\"]))\n",
    "        \n",
    "        # Create 8-shot prompt\n",
    "        prompt = create_few_shot_prompt(train_examples, question, n_shot=config[\"n_shot\"])\n",
    "        \n",
    "        # Generate response\n",
    "        model_response = generate_answer(model, tokenizer, prompt)\n",
    "        \n",
    "        # Log the full response\n",
    "        print(f\"\\nQuestion {idx+1}: {question}\")\n",
    "        print(f\"Model response: {model_response}\")\n",
    "        \n",
    "        # Extract answer from response\n",
    "        predicted_answer = extract_answer(model_response)\n",
    "        \n",
    "        # Check if correct (allowing for minor floating point differences)\n",
    "        is_correct = False\n",
    "        if predicted_answer is not None and target_answer is not None:\n",
    "            # For integer answers, check exact match\n",
    "            if target_answer.is_integer() and predicted_answer.is_integer():\n",
    "                is_correct = int(predicted_answer) == int(target_answer)\n",
    "            else:\n",
    "                # For floating point, allow small relative error\n",
    "                relative_error = abs(predicted_answer - target_answer) / (abs(target_answer) + 1e-10)\n",
    "                is_correct = relative_error < 0.01  # 1% relative error tolerance\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"target_answer\": target_answer,\n",
    "            \"model_response\": model_response,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"running_accuracy\": correct_count / (idx + 1),\n",
    "            \"example_idx\": idx\n",
    "        })\n",
    "    \n",
    "    # Calculate and log final accuracy\n",
    "    accuracy = correct_count / len(test_set)\n",
    "    print(f\"\\nFinal accuracy: {accuracy:.2%} ({correct_count}/{len(test_set)})\")\n",
    "    \n",
    "    # Log final metrics to wandb\n",
    "    wandb.log({\n",
    "        \"final_accuracy\": accuracy,\n",
    "    })\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_file = f\"deepseek_gsm8k_{config['n_shot']}shot_results.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"Detailed results saved to {results_file}\")\n",
    "    \n",
    "    # Log results file to wandb\n",
    "    wandb.save(results_file)\n",
    "    \n",
    "    return accuracy, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        accuracy, results = evaluate_gsm8k_few_shot()\n",
    "        print(f\"Evaluation completed successfully with accuracy: {accuracy:.2%}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        wandb.log({\"error\": str(e)})\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from utils import get_device, load_model, load_gsm8k, extract_answer, create_cot_prompt, generate_answer_hf, is_correct_check\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"max_length\": 512,  # Increased for few-shot CoT\n",
    "    \n",
    "    # Dataset settings\n",
    "    \"num_samples\": 100,  # Set to None to use the full dataset\n",
    "    \"n_shot\": 4,  # Number of examples for few-shot prompting\n",
    "    \n",
    "    # Generation settings\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"num_beams\": 4,\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"run_name\": f\"gpt2-gsm8k-8shot-cot-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"tags\": [\"gsm8k\", \"evaluation\", \"r1\", \"few-shot\", \"cot\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize Weights & Biases with config\n",
    "wandb.init(\n",
    "    project=\"deepseek-gsm8k-benchmark\", \n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    tags=config[\"tags\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Set device\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_gsm8k():\n",
    "    # Load dataset\n",
    "    train_set, test_set = load_gsm8k(config)\n",
    "    \n",
    "    # Prepare training examples list for few-shot prompting\n",
    "    train_examples = []\n",
    "    for ex in train_set:\n",
    "        train_examples.append({\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"answer\": ex[\"answer\"]\n",
    "        })\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model(config, DEVICE)\n",
    "    \n",
    "    results = []\n",
    "    correct_count = 0\n",
    "    \n",
    "    for idx, example in enumerate(tqdm(test_set, desc=\"Evaluating\")):\n",
    "        question = example[\"question\"]\n",
    "        \n",
    "        # Extract target answer, forcing it to be a float\n",
    "        target_answer = extract_answer(example[\"answer\"])\n",
    "        print(f\"\\n\\n{'-'*80}\")\n",
    "        print(f\"Example {idx+1}:\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Target answer: {target_answer}\")\n",
    "        \n",
    "        # Create and print the full CoT prompt\n",
    "        prompt = create_cot_prompt(train_examples, question, n_shot=config[\"n_shot\"])\n",
    "        print(f\"\\nFull CoT Prompt:\\n{'-'*40}\\n{prompt}\\n{'-'*40}\\n\")\n",
    "\n",
    "        # Generate and print the full model response\n",
    "        model_response = generate_answer_hf(model, tokenizer, prompt, config, DEVICE, model_type=\"gpt2\")\n",
    "        print(f\"\\nFull Model Response:\\n{'-'*40}\\n{model_response}\\n{'-'*40}\\n\")\n",
    "        \n",
    "        # Extract answer from response\n",
    "        predicted_answer = extract_answer(model_response)\n",
    "        print(f\"Extracted predicted answer: {predicted_answer}\")\n",
    "        \n",
    "        # Fix for the integer error: Ensure both are converted to float for comparison\n",
    "        is_correct = is_correct_check(predicted_answer, target_answer)\n",
    "        \n",
    "        print(f\"Is correct? {is_correct}\")\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"target_answer\": target_answer,\n",
    "            \"model_response\": model_response,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"running_accuracy\": correct_count / (idx + 1),\n",
    "            \"example_idx\": idx\n",
    "        })\n",
    "        \n",
    "        # Early stop option for debugging (remove for full evaluation)\n",
    "        if idx == 3 and 'debug' in config and config['debug']:\n",
    "            break\n",
    "    \n",
    "    # Calculate and log final accuracy\n",
    "    accuracy = correct_count / len(results)\n",
    "    print(f\"\\nFinal accuracy: {accuracy:.2%} ({correct_count}/{len(results)})\")\n",
    "    \n",
    "    # Log final metrics to wandb\n",
    "    wandb.log({\n",
    "        \"final_accuracy\": accuracy,\n",
    "    })\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_file = f\"{config['model_name']}_gsm8k_{config['n_shot']}shot_results.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"Detailed results saved to {results_file}\")\n",
    "    \n",
    "    # Log results file to wandb\n",
    "    wandb.save(results_file)\n",
    "    \n",
    "    return accuracy, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        accuracy, results = evaluate_gsm8k()\n",
    "        print(f\"Evaluation completed successfully with accuracy: {accuracy:.2%}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        wandb.log({\"error\": str(e)})\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
